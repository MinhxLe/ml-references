epoch:0, loss: 2034.1680
epoch:1, loss: 2061.5195
epoch:2, loss: 1984.7956
epoch:3, loss: 1987.0494
epoch:4, loss: 2074.1467
epoch:5, loss: 2000.3504
epoch:6, loss: 1952.6798
epoch:7, loss: 1909.9387
epoch:8, loss: 1830.1669
epoch:9, loss: 1822.2024
epoch:10, loss: 1824.0271
epoch:11, loss: 1841.5628
epoch:12, loss: 1815.0964
epoch:13, loss: 1846.5585
epoch:14, loss: 1800.7726
/Users/minh/project/ml-references/main.py:133: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)
  if batch_data.grad is not None:
epoch:15, loss: 1864.8283
epoch:16, loss: 1826.6353
epoch:17, loss: 1784.3267
epoch:18, loss: 1710.4460
epoch:19, loss: 1681.2611
epoch:20, loss: 1739.8769
epoch:21, loss: 1638.8673
epoch:22, loss: 1693.9346
epoch:23, loss: 1711.1526
epoch:24, loss: 1723.1073
epoch:25, loss: 1765.6245
epoch:26, loss: 1725.7279
epoch:27, loss: 1710.5764
epoch:28, loss: 1758.5043
epoch:29, loss: 1655.5017
epoch:30, loss: 1720.7667
epoch:31, loss: 1745.0047
epoch:32, loss: 1653.1024
epoch:33, loss: 1726.6189
epoch:34, loss: 1632.6549
epoch:35, loss: 1692.7259
epoch:36, loss: 1685.0137
epoch:37, loss: 1717.3177
epoch:38, loss: 1626.9743
epoch:39, loss: 1649.3445
epoch:40, loss: 1653.7528
epoch:41, loss: 1728.0938
epoch:42, loss: 1684.0891
epoch:43, loss: 1694.3071
epoch:44, loss: 1640.6260
epoch:45, loss: 1653.5692
epoch:46, loss: 1675.6240
epoch:47, loss: 1708.7422
epoch:48, loss: 1656.4209
epoch:49, loss: 1729.7454
epoch:50, loss: 1636.4611
epoch:51, loss: 1675.8551
epoch:52, loss: 1585.4833
epoch:53, loss: 1686.8553
epoch:54, loss: 1699.7881
epoch:55, loss: 1698.6394
epoch:56, loss: 1675.1334
epoch:57, loss: 1683.3871
epoch:58, loss: 1662.0095
epoch:59, loss: 1721.7906
epoch:60, loss: 1659.0772
epoch:61, loss: 1657.5702
epoch:62, loss: 1647.8623
epoch:63, loss: 1657.8576
epoch:64, loss: 1704.6418
epoch:65, loss: 1703.7139
epoch:66, loss: 1640.7065
epoch:67, loss: 1711.1223
epoch:68, loss: 1670.3064
epoch:69, loss: 1638.7782
epoch:70, loss: 1712.2478
epoch:71, loss: 1625.3708
epoch:72, loss: 1671.0635
epoch:73, loss: 1696.5979
epoch:74, loss: 1696.6526
epoch:75, loss: 1650.8635
epoch:76, loss: 1706.0954
epoch:77, loss: 1622.7609
epoch:78, loss: 1675.5221
epoch:79, loss: 1644.5595
epoch:80, loss: 1690.9271
epoch:81, loss: 1629.9823
epoch:82, loss: 1645.4153
epoch:83, loss: 1632.7286
epoch:84, loss: 1752.1908
epoch:85, loss: 1717.8631
epoch:86, loss: 1634.3962
epoch:87, loss: 1683.4116
epoch:88, loss: 1648.1510
epoch:89, loss: 1657.5390
epoch:90, loss: 1645.1685
epoch:91, loss: 1704.3744
epoch:92, loss: 1658.5328
epoch:93, loss: 1673.8883
epoch:94, loss: 1649.9285
epoch:95, loss: 1675.5485
epoch:96, loss: 1678.3154
epoch:97, loss: 1685.2263
epoch:98, loss: 1655.2382
epoch:99, loss: 1676.0463
epoch:100, loss: 1701.7676
epoch:101, loss: 1627.9151
epoch:102, loss: 1706.8208
epoch:103, loss: 1670.8338
epoch:104, loss: 1670.6634
epoch:105, loss: 1698.6522
epoch:106, loss: 1727.9857
epoch:107, loss: 1667.0725
epoch:108, loss: 1696.2732
epoch:109, loss: 1682.9437
epoch:110, loss: 1727.9098
epoch:111, loss: 1664.5775
epoch:112, loss: 1662.5201
epoch:113, loss: 1626.5653
epoch:114, loss: 1691.0094
epoch:115, loss: 1708.5058
epoch:116, loss: 1647.9595
epoch:117, loss: 1725.3367
epoch:118, loss: 1637.0863
epoch:119, loss: 1647.5013
epoch:120, loss: 1654.3989
epoch:121, loss: 1687.8752
epoch:122, loss: 1674.8298
epoch:123, loss: 1656.3962
epoch:124, loss: 1752.3165
epoch:125, loss: 1630.6863
epoch:126, loss: 1663.1735
epoch:127, loss: 1715.3434
epoch:128, loss: 1658.5495
epoch:129, loss: 1658.9311
epoch:130, loss: 1694.8032
epoch:131, loss: 1712.7157
epoch:132, loss: 1626.5748
epoch:133, loss: 1684.9065
epoch:134, loss: 1667.6620
epoch:135, loss: 1689.7109
epoch:136, loss: 1710.2481
epoch:137, loss: 1681.9837
epoch:138, loss: 1644.6029
epoch:139, loss: 1729.7313
epoch:140, loss: 1703.2456
epoch:141, loss: 1709.7299
epoch:142, loss: 1672.5528
epoch:143, loss: 1744.3427
epoch:144, loss: 1649.9397
epoch:145, loss: 1675.8943
epoch:146, loss: 1719.1925
epoch:147, loss: 1660.5446
epoch:148, loss: 1653.6235
epoch:149, loss: 1647.1797
epoch:150, loss: 1606.8325
epoch:151, loss: 1694.9877
epoch:152, loss: 1600.5120
epoch:153, loss: 1600.4892
epoch:154, loss: 1604.3639
epoch:155, loss: 1651.6855
epoch:156, loss: 1667.1015
epoch:157, loss: 1597.3128
epoch:158, loss: 1644.2149
epoch:159, loss: 1633.1512
epoch:160, loss: 1637.8842
epoch:161, loss: 1675.1810
epoch:162, loss: 1636.4197
epoch:163, loss: 1680.3347
epoch:164, loss: 1651.5705
epoch:165, loss: 1694.6685
epoch:166, loss: 1710.8664
epoch:167, loss: 1658.9994
epoch:168, loss: 1749.8197
epoch:169, loss: 1677.0286
epoch:170, loss: 1638.7940
epoch:171, loss: 1677.0600
epoch:172, loss: 1626.4709
epoch:173, loss: 1673.0991
epoch:174, loss: 1653.0182
epoch:175, loss: 1668.6505
epoch:176, loss: 1631.7130
epoch:177, loss: 1659.9938
epoch:178, loss: 1747.4337
epoch:179, loss: 1715.2207
epoch:180, loss: 1658.7214
epoch:181, loss: 1754.7205
epoch:182, loss: 1666.5565
epoch:183, loss: 1643.8693
epoch:184, loss: 1658.1340
epoch:185, loss: 1635.0130
epoch:186, loss: 1614.7920
epoch:187, loss: 1716.9213
epoch:188, loss: 1677.0477
epoch:189, loss: 1668.9138
epoch:190, loss: 1668.6890
epoch:191, loss: 1680.1333
epoch:192, loss: 1636.9459
epoch:193, loss: 1723.6487
epoch:194, loss: 1631.9514
epoch:195, loss: 1742.2588
epoch:196, loss: 1697.0772
epoch:197, loss: 1665.5922
epoch:198, loss: 1623.7340
epoch:199, loss: 1683.7402
epoch:200, loss: 1613.4332
epoch:201, loss: 1720.1284
epoch:202, loss: 1636.7850
epoch:203, loss: 1671.8230
epoch:204, loss: 1653.3270
epoch:205, loss: 1685.0085
epoch:206, loss: 1695.8616
epoch:207, loss: 1679.4092
epoch:208, loss: 1683.3549
epoch:209, loss: 1689.6180
epoch:210, loss: 1672.8478
epoch:211, loss: 1699.3192
epoch:212, loss: 1663.9272
epoch:213, loss: 1652.4158
epoch:214, loss: 1703.5111
epoch:215, loss: 1612.2511
epoch:216, loss: 1712.1937
epoch:217, loss: 1671.2419
epoch:218, loss: 1704.2126
epoch:219, loss: 1644.7911
epoch:220, loss: 1628.4665
epoch:221, loss: 1609.7887
epoch:222, loss: 1648.2038
epoch:223, loss: 1637.8081
epoch:224, loss: 1594.1278
epoch:225, loss: 1610.1755
epoch:226, loss: 1622.7088
epoch:227, loss: 1712.7967
epoch:228, loss: 1673.7120
epoch:229, loss: 1635.5608
epoch:230, loss: 1679.3271
epoch:231, loss: 1680.2175
epoch:232, loss: 1680.2637
epoch:233, loss: 1640.8013
epoch:234, loss: 1602.4332
epoch:235, loss: 1651.3658
epoch:236, loss: 1642.8508
epoch:237, loss: 1648.0154
epoch:238, loss: 1608.1356
epoch:239, loss: 1679.3644
epoch:240, loss: 1632.6113
epoch:241, loss: 1619.4973
epoch:242, loss: 1646.5517
epoch:243, loss: 1676.6955
epoch:244, loss: 1587.7017
epoch:245, loss: 1661.8697
epoch:246, loss: 1638.4718
epoch:247, loss: 1622.5928
epoch:248, loss: 1624.1771
epoch:249, loss: 1651.4447